{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d86fb85a",
   "metadata": {},
   "source": [
    "# Capstone Journey Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4d969f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "from sklearn.metrics import roc_auc_score, brier_score_loss, precision_recall_fscore_support, confusion_matrix\n",
    "from xgboost import XGBClassifier\n",
    "import shap, matplotlib.pyplot as plt, joblib, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(42)\n",
    "\n",
    "DATA_PATH = Path(\"data/diabetes_prediction_dataset.csv\")\n",
    "MODELS_DIR = Path(\"models\"); MODELS_DIR.mkdir(exist_ok=True)\n",
    "RESULTS_DIR = Path(\"results\"); RESULTS_DIR.mkdir(exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999b69e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6b7f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape, df.describe(include=\"all\").T.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1664cd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"class balance:\"); \n",
    "print(df[\"diabetes\"].value_counts(normalize=True).round(3))\n",
    "print(\"\\nmissing fraction (top 10):\")\n",
    "print(df.isna().mean().sort_values(ascending=False).head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f86ef25",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = [c for c in df.select_dtypes(include=[np.number]).columns if c != \"diabetes\"]\n",
    "if num_cols:\n",
    "    fig, ax = plt.subplots(figsize=(6,4))\n",
    "    df[num_cols[0]].hist(bins=30, ax=ax); ax.set_title(f\"Histogram: {num_cols[0]}\"); plt.show()\n",
    "if len(num_cols) > 1:\n",
    "    fig, ax = plt.subplots(figsize=(6,4))\n",
    "    df[num_cols[1]].hist(bins=30, ax=ax); ax.set_title(f\"Histogram: {num_cols[1]}\"); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3af6761",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(num_cols) >= 2:\n",
    "    corr = df[num_cols].corr(numeric_only=True)\n",
    "    fig, ax = plt.subplots(figsize=(6,5))\n",
    "    im = ax.imshow(corr.values, aspect=\"auto\")\n",
    "    ax.set_xticks(range(len(num_cols))); ax.set_xticklabels(num_cols, rotation=90)\n",
    "    ax.set_yticks(range(len(num_cols))); ax.set_yticklabels(num_cols)\n",
    "    ax.set_title(\"Correlation (numeric)\"); plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1835145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_smoking(d: pd.DataFrame) -> pd.DataFrame:\n",
    "    d = d.copy()\n",
    "    col = \"smoking_history\"\n",
    "    if col in d.columns:\n",
    "        v = d[col].astype(str).str.strip().str.lower()\n",
    "        v = v.replace({\"ever\":\"former\",\"not current\":\"former\",\"no info\":\"unknown\",\"none\":\"unknown\",\"nan\":\"unknown\"})\n",
    "        v = v.where(v.isin([\"never\",\"former\",\"current\",\"unknown\"]), \"unknown\")\n",
    "        d[col] = v\n",
    "        d[\"ever_smoked\"] = v.isin([\"former\",\"current\"]).astype(int)\n",
    "    else:\n",
    "        d[\"smoking_history\"] = \"unknown\"\n",
    "        d[\"ever_smoked\"] = 0\n",
    "    return d\n",
    "\n",
    "df = normalize_smoking(df)\n",
    "y = df[\"diabetes\"].astype(int)\n",
    "X = df.drop(columns=[\"diabetes\"])\n",
    "\n",
    "numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "binary_features = [c for c in [\"hypertension\",\"heart_disease\",\"ever_smoked\"] if c in X.columns]\n",
    "cat_features = [c for c in X.select_dtypes(include=[\"object\",\"category\"]).columns if c not in binary_features]\n",
    "\n",
    "numeric_tf = Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())])\n",
    "binary_tf = \"passthrough\"\n",
    "cat_tf = Pipeline([(\"imputer\", SimpleImputer(strategy=\"most_frequent\")), (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\"))])\n",
    "\n",
    "preprocessor = ColumnTransformer([(\"num\", numeric_tf, numeric_features),\n",
    "                                  (\"bin\", binary_tf, binary_features),\n",
    "                                  (\"cat\", cat_tf, cat_features)], remainder=\"drop\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe45aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42, stratify=y)\n",
    "\n",
    "models = {\n",
    "    \"logistic\": LogisticRegression(max_iter=2000, solver=\"lbfgs\"),\n",
    "    \"rf\": RandomForestClassifier(n_estimators=400, random_state=42, n_jobs=-1),\n",
    "    \"xgb\": XGBClassifier(n_estimators=300, learning_rate=0.05, max_depth=4,\n",
    "                         subsample=0.9, colsample_bytree=0.9, reg_lambda=1.0,\n",
    "                         reg_alpha=0.0, n_jobs=-1, objective=\"binary:logistic\",\n",
    "                         eval_metric=\"logloss\", random_state=42),\n",
    "}\n",
    "\n",
    "rows = []\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for name, clf in models.items():\n",
    "    pipe_m = Pipeline([(\"preprocessor\", preprocessor), (\"classifier\", clf)])\n",
    "    aucs = cross_val_score(pipe_m, X_train, y_train, cv=cv, scoring=\"roc_auc\")\n",
    "    rows.append([name, aucs.mean(), aucs.std()])\n",
    "\n",
    "cmp = pd.DataFrame(rows, columns=[\"model\",\"cv_auc_mean\",\"cv_auc_std\"]).sort_values(\"cv_auc_mean\", ascending=False).reset_index(drop=True)\n",
    "print(cmp)\n",
    "best_name = cmp.loc[0,\"model\"]; print(\"keep:\", best_name); print(\"drop:\", cmp.loc[1:,\"model\"].tolist())\n",
    "\n",
    "clf_best = models[best_name]\n",
    "pipe = Pipeline([(\"preprocessor\", preprocessor), (\"classifier\", clf_best)])\n",
    "pipe.fit(X_train, y_train)\n",
    "proba_test = pipe.predict_proba(X_test)[:,1]\n",
    "print(\"test AUC (uncalibrated):\", round(roc_auc_score(y_test, proba_test), 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc9a03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if best_name == \"xgb\":\n",
    "    grid = [\n",
    "        dict(max_depth=3, n_estimators=250, learning_rate=0.07, subsample=0.9, colsample_bytree=0.9),\n",
    "        dict(max_depth=4, n_estimators=300, learning_rate=0.05, subsample=0.9, colsample_bytree=0.9),\n",
    "        dict(max_depth=5, n_estimators=350, learning_rate=0.04, subsample=0.9, colsample_bytree=0.8),\n",
    "    ]\n",
    "    best_auc = -1.0; best_params = None\n",
    "    for p in grid:\n",
    "        xgb_t = XGBClassifier(**p, reg_lambda=1.0, reg_alpha=0.0, n_jobs=-1,\n",
    "                              objective=\"binary:logistic\", eval_metric=\"logloss\", random_state=42)\n",
    "        pipe_t = Pipeline([(\"preprocessor\", preprocessor), (\"classifier\", xgb_t)])\n",
    "        aucs = cross_val_score(pipe_t, X_train, y_train, cv=cv, scoring=\"roc_auc\")\n",
    "        m = aucs.mean(); print(\"try\", p, \"cv_auc:\", round(m,3))\n",
    "        if m > best_auc: best_auc, best_params = m, p\n",
    "    if best_params is not None:\n",
    "        print(\"tuned:\", best_params, \"| cv_auc:\", round(best_auc,3))\n",
    "        clf_best = XGBClassifier(**best_params, reg_lambda=1.0, reg_alpha=0.0, n_jobs=-1,\n",
    "                                 objective=\"binary:logistic\", eval_metric=\"logloss\", random_state=42)\n",
    "        pipe = Pipeline([(\"preprocessor\", preprocessor), (\"classifier\", clf_best)])\n",
    "        pipe.fit(X_train, y_train)\n",
    "        proba_test = pipe.predict_proba(X_test)[:,1]\n",
    "        print(\"test AUC (uncalibrated, tuned):\", round(roc_auc_score(y_test, proba_test), 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738d15d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cal = CalibratedClassifierCV(pipe, method=\"isotonic\", cv=5)\n",
    "cal.fit(X_train, y_train)\n",
    "\n",
    "proba_uncal = pipe.predict_proba(X_test)[:,1]\n",
    "proba_cal = cal.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(\"brier uncal:\", round(brier_score_loss(y_test, proba_uncal), 4))\n",
    "print(\"brier cal  :\", round(brier_score_loss(y_test, proba_cal), 4))\n",
    "\n",
    "prob_true_uncal, prob_pred_uncal = calibration_curve(y_test, proba_uncal, n_bins=10, strategy=\"quantile\")\n",
    "prob_true_cal, prob_pred_cal = calibration_curve(y_test, proba_cal, n_bins=10, strategy=\"quantile\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,5))\n",
    "ax.plot([0,1],[0,1],\"--\", alpha=0.5)\n",
    "ax.plot(prob_pred_uncal, prob_true_uncal, marker=\"o\", label=\"uncal\")\n",
    "ax.plot(prob_pred_cal, prob_true_cal, marker=\"o\", label=\"cal\")\n",
    "ax.set_xlabel(\"predicted\"); ax.set_ylabel(\"observed\"); ax.set_title(\"Calibration\"); ax.legend()\n",
    "fig.tight_layout(); fig.savefig(RESULTS_DIR / \"calibration.png\", dpi=150); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0011800d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ths = np.linspace(0.05, 0.95, 19)\n",
    "rows = []; best = (None, -1.0)\n",
    "for t in ths:\n",
    "    y_hat = (proba_cal >= t).astype(int)\n",
    "    p, r, f1, _ = precision_recall_fscore_support(y_test, y_hat, average=\"binary\", zero_division=0)\n",
    "    rows.append([t, p, r, f1])\n",
    "    if f1 > best[1]: best = (t, f1)\n",
    "\n",
    "sweep = pd.DataFrame(rows, columns=[\"threshold\",\"precision\",\"recall\",\"f1\"])\n",
    "best_t, best_f1 = best\n",
    "print(\"best threshold:\", round(best_t,3), \"f1:\", round(best_f1,3))\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "ax.plot(sweep[\"threshold\"], sweep[\"f1\"], marker=\"o\"); ax.set_xlabel(\"threshold\"); ax.set_ylabel(\"F1\"); ax.set_title(\"Threshold sweep\")\n",
    "fig.tight_layout(); fig.savefig(RESULTS_DIR / \"threshold_curve.png\", dpi=150); plt.show()\n",
    "\n",
    "cm = confusion_matrix(y_test, (proba_cal >= best_t).astype(int))\n",
    "cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9983332",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = pipe.named_steps[\"classifier\"]\n",
    "pp = pipe.named_steps[\"preprocessor\"]\n",
    "Xtr = pp.transform(X_train)\n",
    "try:\n",
    "    explainer = shap.TreeExplainer(clf)\n",
    "    sv = explainer(Xtr[:1])\n",
    "    shap.plots.waterfall(sv[0], show=False)\n",
    "    plt.gcf().set_size_inches(8,5); plt.tight_layout()\n",
    "    plt.savefig(RESULTS_DIR / \"shap_waterfall.png\", dpi=150); plt.show()\n",
    "except Exception as e:\n",
    "    print(\"SHAP warning:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e33858",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = MODELS_DIR / \"calibrated_diabetes_model_v5.pkl\"\n",
    "joblib.dump(cal, out_path); out_path\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
